{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBhV_GzngOxT",
        "outputId": "300eacb5-646d-447d-821d-2a94d4e8ba9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import os\n",
        "! pip install -r requirements.txt -q\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats\n",
        "# from langchain.agents import create_pandas_dataframe_agent\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import(\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n"
      ],
      "metadata": {
        "id": "mi3hAzVjgk7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"\"\"\n",
        "Mojo combines the usability of Python with the performance of C, unlocking unparalleled programmability \\\n",
        "of AI hardware and extensibility of AI models.\n",
        "Mojo is a new programming language that bridges the gap between research and production \\\n",
        "by combining the best of Python syntax with systems programming and metaprogramming.\n",
        "With Mojo, you can write portable code that’s faster than C and seamlessly inter-op with the Python ecosystem.\n",
        "When we started Modular, we had no intention of building a new programming language. \\\n",
        "But as we were building our platform with the intent to unify the world’s ML/AI infrastructure, \\\n",
        "we realized that programming across the entire stack was too complicated. Plus, we were writing a \\\n",
        "lot of MLIR by hand and not having a good time.\n",
        "And although accelerators are important, one of the most prevalent and sometimes overlooked \"accelerators\" \\\n",
        "is the host CPU. Nowadays, CPUs have lots of tensor-core-like accelerator blocks and other AI acceleration \\\n",
        "units, but they also serve as the “fallback” for operations that specialized accelerators don’t handle, \\\n",
        "such as data loading, pre- and post-processing, and integrations with foreign systems. \\\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content='You are an expert copywriter with expertize in summarizing documents'),\n",
        "    HumanMessage(content=f'Please provide a short and concise summary of the following text:\\n TEXT: {text}')\n",
        "]\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
        "\n"
      ],
      "metadata": {
        "id": "E3Tyz2NujvX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(text)\n"
      ],
      "metadata": {
        "id": "vLIgRexojzjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ec26c3-ecb6-4753-d264-2f847363c7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "229"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_output = llm(messages)"
      ],
      "metadata": {
        "id": "m9H8poMrm1uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "Va_5-ywTm7Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = '''\n",
        "Write a concise and short summary of the following text:\n",
        "TEXT: `{text}`\n",
        "Translate the summary to {language}.\n",
        "'''\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['text', 'language'],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "2P5OR6ZYm7f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(prompt.format(text=text, language='English'))"
      ],
      "metadata": {
        "id": "fJHUMzWXm7jB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d779f4-e211-42d7-d0a6-e1ed96311713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "summary = chain.run({'text': text, 'language':'marathi'})\n",
        "summary"
      ],
      "metadata": {
        "id": "S8fJmSw0m7l7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "405976f6-451d-4a8d-cd69-8b6fd3259a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'मोजो हे पायथनच्या उपयोगकर्तांसाठी आणि सीच्या प्रदर्शनाच्या संयमाने संयुक्त करून, एआय हार्डवेअरची अद्वितीय प्रोग्रॅमेबिलिटी आणि एआय मॉडेल्सची विस्तारणीयता उघडते.\\nमोजो हे एक नवीन प्रोग्रॅमिंग भाषा आहे ज्याने संशोधन आणि उत्पादनाच्या दरम्यान अंतराची गाप भरते, पायथन सिंटॅक्सच्या सर्वोत्तम संयुक्त करून सिस्टम्स प्रोग्रॅमिंग आणि मेटाप्रोग्रॅमिंगच्या उत्तम गुणांचा संयोजन करते.\\nमोजोसह, आपण सीच्यापेक्षा जलद कोड लिहू शकता आणि पायथन इकोसिस्टमसह सुसंगतपणे कार्य करू शकता.\\nमोड्युलर सुरू केल्यावर आम्हाला एक नवीन प्रोग्रॅमिंग भाषा तयार करण्याची इच्छा नव्हती. पण आम्ही जगातील एमएल / एआय इंफ्रास्ट्रक्चर समाहित करण्याच्या इरद्याने आम्ही कळाल की संपूर्ण स्टॅकवर प्रोग्रॅमिंग करणे खूप कंप्लिकेटेड आहे. व आम्ही खूप MLIR हाताने लिहित होतो आणि चांगला वेळ नाही होता.\\nआणि ज्यांच्यासाठी एक्सेलरेटर्स महत्त्वाचे आहेत, त्यांच्यामध्ये एक अत्यंत प्रसिद्ध आणि कधीकधी दुर्लक्षित \"एक्सेलरेटर\" म्हणजे होस्ट सीपीयू आहे. आजकाल, सीपीयूसह टेन्सर-कोर-सारख्या एक्सेलरेटर ब्लॉक्स आणि इतर एआय एक्सेलरेशन युनिट्स असतात, पण त्यांना डेटा लोडिंग, प्री- आणि पोस्ट-प्रोसेसिंग, आणि विदेशी सिस्टमसह संयोगात आणि इतर ऑपरेशन्ससाठी \"फॉलबॅक\" म्हणून सेवा करतात.`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n"
      ],
      "metadata": {
        "id": "W5Y_bVMUm7pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vk.txt', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# text\n",
        "\n",
        "docs = [Document(page_content=text)]\n",
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "BfAIUm87qchU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = '''Write a concise and short summary of the following text.\n",
        "TEXT: `{text}`\n",
        "'''\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['text'],\n",
        "    template=template\n",
        ")"
      ],
      "metadata": {
        "id": "c4TqXxC8w9CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMoLTnZsxBJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    prompt=prompt,\n",
        "    verbose=False\n",
        ")\n",
        "output_summary = chain.run(docs)"
      ],
      "metadata": {
        "id": "-N8bVrHAxSCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "id": "NHUWJ9n9yKzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691fb0bf-8f35-46b3-e874-78af492cf562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virat Kohli is an Indian cricketer widely regarded as one of the greatest batsmen in the sport. He has achieved numerous records and accolades, including being named the male cricketer of the decade by the International Cricket Council. Kohli's career highlights include winning the 2011 Cricket World Cup and 2013 ICC Champions Trophy. He has also been praised for his performances in various tournaments, such as the Emerging Players Tournament in Australia in 2009. Kohli's success on the field has been attributed to his maturity and humility, despite initial perceptions of him as brash and arrogant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Summarizing Large Documents Using map_reduce"
      ],
      "metadata": {
        "id": "pwClv8gRyNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "eaNWWSAl3rz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('vk.txt', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "MLAbewJY3sxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(text)"
      ],
      "metadata": {
        "id": "psbZKA9d33sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4851412f-0e25-4d35-9b8e-9b4e85f9baa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "848"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "GX7m9fSu36oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "EFHpp-Z5380L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c581685b-5c6c-4261-a551-99d4bdaf0758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type='map_reduce',\n",
        "    verbose=False\n",
        ")\n",
        "output_summary = chain.run(chunks)"
      ],
      "metadata": {
        "id": "FXeCygSl3-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "id": "Ml8AMJuy4Beq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df2dd67-95d5-45c0-dc96-67e93f20c7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virat Kohli is an Indian cricketer considered one of the greatest batsmen in history, holding records in T20I and IPL. He was named male cricketer of the decade in 2020 and played a key role in India's victories in the 2011 Cricket World Cup and 2013 ICC Champions Trophy. Kohli's career began with a standout performance in the 2009 Emerging Players Tournament, and he is praised for his maturity and talent in international cricket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.llm_chain.prompt.template"
      ],
      "metadata": {
        "id": "DkX_pytp4D_G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f7945bf9-9fd5-4d42-db6f-3a3db2a7a4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.combine_document_chain.llm_chain.prompt.template"
      ],
      "metadata": {
        "id": "DiXz6KAm4HdR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ed0e94af-5e97-47cf-ceb8-873fe74904a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = '''\n",
        "Write a short and concise summary of the following:\n",
        "Text: `{text}`\n",
        "CONCISE SUMMARY:\n",
        "'''\n",
        "map_prompt_template = PromptTemplate(\n",
        "    input_variables=['text'],\n",
        "    template=map_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "xCLDCwMa4K3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_prompt = '''\n",
        "Write a concise summary of the following text that covers the key points.\n",
        "Add a title to the summary.\n",
        "Start your summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED\n",
        "by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
        "Text: `{text}`\n",
        "'''\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=['text'])"
      ],
      "metadata": {
        "id": "xR6H1k1B4Om5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### map_reduce wich Custom Prompts"
      ],
      "metadata": {
        "id": "9ZOBbGP74bpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='map_reduce',\n",
        "    map_prompt=map_prompt_template,\n",
        "    combine_prompt=combine_prompt_template,\n",
        "    verbose=False\n",
        ")\n",
        "output = summary_chain.run(chunks)"
      ],
      "metadata": {
        "id": "cVZv3S9d4RkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "an-pvQRj4UUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcede93-1d7a-4f88-dba6-6819632807ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Virat Kohli: The Legendary Indian Cricketer\n",
            "\n",
            "Introduction:\n",
            "Virat Kohli is a highly acclaimed Indian cricketer known for his exceptional batting skills and numerous records in the sport.\n",
            "\n",
            "Key Points:\n",
            "- Widely regarded as one of the greatest batsmen in cricket history\n",
            "- Holds records for being the highest run-scorer in T20I and IPL\n",
            "- Named male cricketer of the decade by the International Cricket Council in 2020\n",
            "- Played a crucial role in India's victories in the 2011 Cricket World Cup and 2013 ICC Champions Trophy\n",
            "- Career took off after a standout performance in the 2009 Emerging Players Tournament\n",
            "- Praised for his maturity and humble attitude by teammates and captain MS Dhoni\n",
            "\n",
            "Conclusion:\n",
            "Virat Kohli's remarkable achievements and talent have solidified his status as a legendary figure in the world of cricket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Summarizing Using the refine Chain"
      ],
      "metadata": {
        "id": "xKgnuwHU4Wmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader"
      ],
      "metadata": {
        "id": "p7U9kvqz4kDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsB5D_dU4m8E",
        "outputId": "0ba7f1d8-d027-4c8a-ed45-b31e89813278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured -q"
      ],
      "metadata": {
        "id": "8gjG8C134qAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image\n",
        "!pip install pdfminer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrSpmlYu4s2G",
        "outputId": "0e8d1c77-8d03-40ba-8db2-39c2e0b6bc85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.10/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from pdfminer) (3.20.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loader = UnstructuredPDFLoader('Plant diseases by leaf classification.pdf')\n",
        "# data = loader.load()"
      ],
      "metadata": {
        "id": "_r5Vde4p4yn7"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a8vvcwO5VHu",
        "outputId": "28d67fb2-42ba-4fbe-b691-34b18cb55cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7c36f3506ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
        "# chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "-02nheQ97FtA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "j9Rb1Fll7R9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "wZhtibXb73nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.002:.6f}')\n",
        "\n",
        "\n",
        "print_embedding_cost(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XQxzgoT73qf",
        "outputId": "cca1f05c-352c-4f03-bc4e-54a13a519ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 848\n",
            "Embedding Cost in USD: 0.001696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='refine',\n",
        "    verbose=True\n",
        ")\n",
        "output_summary = chain.run(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "093a-oGT736D",
        "outputId": "4789ff9e-13f2-489a-ee02-a60005d4f548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"irat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] ⓘ; born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team. He currently represents Royal Challengers Bangalore in the IPL and Delhi in domestic cricket. Kohli is widely regarded as one of the greatest batsmen in the history of the sport.[3] He is the highest run scorer in T20I and IPL. In 2020, the International Cricket Council named him the male cricketer of the decade. Kohli is currently fourth-highest run-scorer in international cricket and stands second in the list of most international centuries scored. He also holds the record for scoring the most centuries in One Day International cricket.[4][5] Kohli was a member of the Indian team that won the 2011 Cricket World Cup and 2013 ICC Champions Trophy.\n",
            "\n",
            "\n",
            "In July–August 2009, Kohli was selected in the four-team Emerging Players Tournament, held in Australia. He was selected to open the innings for the Indian Emerging Players team in the tournament, and he went on to have a standout performance. Kohli finished as the tournament's leading run-scorer, with a total of 398 runs from seven matches, at an average of 66.33. He was particularly impressive in the final match, where he scored 104 runs off 102 balls against the South Africa Emerging Players team in Brisbane. His strong performance helped lead his team to a 17-run victory and the tournament title.[57][58] At the conclusion of the tournament, Kris Srikkanth, the Chairman of the Indian national selection committee, expressed his admiration for Kohli's performance during the tournament. Srikkanth stated, \"I must say, opener Virat Kohli was outstanding. Some of the shots he played spoke about his ability.\"[59] Kohli himself has stated that this tournament was a \"turning point\" in his career.[60]\n",
            "\n",
            "In August 2009, Kohli returned to the national team after recovering from a minor shoulder injury, replacing the injured Gautam Gambhir in the Indian squad for the tri-series in Sri Lanka.[61] He was also utilized as a middle order batsman in the 2009 ICC Champions Trophy due to an injury sustained by Yuvraj Singh.[62] In December of that same year, he was included in the team for home ODI series against Sri Lanka and scored 27[63] and 54 in the first two ODIs before making way for Yuvraj, who regained fitness for the third ODI. However, due to the reoccurrence of a finger injury, Yuvraj was ruled out indefinitely,[64] which led to Kohli's return to the team in the fourth ODI at Kolkata. In that match, Kohli scored his maiden ODI century–107 off 114 balls–while sharing a 224-run partnership for the third wicket with Gambhir. As a result of this performance, India won by seven wickets and sealed the series 3–1.[65][b]\n",
            "\n",
            "2010–2011: Rise through the ranks\n",
            "In January 2010, Kohli was given the opportunity in tri-nation ODI tournament in Bangladesh, as Tendulkar was rested for the event.[66] During the series, Kohli became just the third Indian player to score two ODI centuries before the age of 22.[67] He was widely hailed for his performances, and ultimately emerged as the leading run-scorer of the series, with 275 runs from five innings at an impressive average of 91.66.[68] Kohli's success on the field belies the stereotype of him as a brash and arrogant player. In fact, Indian captain MS Dhoni has noted that \"he has grabbed his chances\" and that \"he has matured now.\" Dhoni went on to say that \"To us, he comes as a 'humble guy'. He might come across different to the world.\"[69]\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmSFvzvD739H",
        "outputId": "cda4385a-b63f-4eee-ac49-d70626aa99d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virat Kohli is an Indian cricketer known for his exceptional batting skills. He has achieved numerous records in international cricket, including being the highest run-scorer in T20I and IPL. Kohli was named the male cricketer of the decade by the International Cricket Council in 2020. He was a key player in India's victories at the 2011 Cricket World Cup and 2013 ICC Champions Trophy. Kohli's career took off after a standout performance in the 2009 Emerging Players Tournament in Australia. He has been praised for his maturity and humility by his teammates and captain MS Dhoni.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise summary of the following extracting the key information:\n",
        "Text: `{text}`\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "initial_prompt = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
        "\n",
        "refine_template = '''\n",
        "    Your job is to produce a final summary.\n",
        "    I have provided an existing summary up to a certain point: {existing_answer}.\n",
        "    Please refine the existing summary with some more context below.\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "    Start the final summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED\n",
        "    by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
        "\n",
        "'''\n",
        "refine_prompt = PromptTemplate(\n",
        "    template=refine_template,\n",
        "    input_variables=['existing_answer', 'text']\n",
        ")\n"
      ],
      "metadata": {
        "id": "aT6rvEj273_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type='refine',\n",
        "    question_prompt=initial_prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        "    return_intermediate_steps=False\n",
        "\n",
        ")\n",
        "output_summary = chain.run(chunks)"
      ],
      "metadata": {
        "id": "Fj4Kh-ba74C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO6V-J6g74Gj",
        "outputId": "c92822dd-2a45-4a94-c605-5f4c08f5b3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virat Kohli is an Indian cricketer known for his exceptional batting skills. He has represented India in various tournaments and holds records for the highest run scorer in T20I and IPL. Kohli was named the male cricketer of the decade by the International Cricket Council in 2020. He was a key player in India's victories in the 2011 Cricket World Cup and 2013 ICC Champions Trophy. Kohli's career took off after a standout performance in the 2009 Emerging Players Tournament in Australia. He has been praised for his maturity and talent by his teammates and has emerged as one of the greatest batsmen in the history of the sport.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "Ydj3t9qt8L_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXkMbJmN8Q0M",
        "outputId": "8de3ae01-2d4e-45f8-aef7-3925eae89cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
        "wikipedia = WikipediaAPIWrapper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "utTnUoZp8Tc5",
        "outputId": "d926407a-7148-4b17-fff6-9d9838752a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Could not import wikipedia python package. Please install it with `pip install wikipedia`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/wikipedia.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wikipedia'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-bd36e72fecfd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwikipedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipediaAPIWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROOT_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/wikipedia.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wiki_client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0;34m\"Could not import wikipedia python package. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;34m\"Please install it with `pip install wikipedia`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import wikipedia python package. Please install it with `pip install wikipedia`.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Wikipedia\",\n",
        "        func=wikipedia.run,\n",
        "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lJgD_dQF8Vr5",
        "outputId": "04ed7936-2eb7-41ca-ac0d-72714437e36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wikipedia' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-58e996ad5b20>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     Tool(\n\u001b[1;32m      3\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Wikipedia\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Useful for when you need to get information from wikipedia about a single topic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wikipedia' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "rGg5Pc288cAw",
        "outputId": "904e597c-dcdb-43c6-9792-2d214576e233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tools' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-a3c03b23b194>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zero-shot-react-description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tools' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = agent_executor.run('Can you please provide a short summary of George Washington?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "-iGotyDk8fq7",
        "outputId": "ddd92647-96ac-4061-cdd8-dcb8a176497e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent_executor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-b6319282751f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Can you please provide a short summary of George Washington?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent_executor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRY0cItW8i0B",
        "outputId": "48ab2ed4-2736-438a-fc66-815d278c7fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Virat Kohli: A Cricketing Legend\n",
            "\n",
            "Introduction:\n",
            "Virat Kohli is a renowned Indian cricketer known for his exceptional batting skills and numerous records in international cricket.\n",
            "\n",
            "Key Points:\n",
            "- Highest run-scorer in T20I and IPL\n",
            "- Named male cricketer of the decade by ICC in 2020\n",
            "- Played crucial role in India's victories in 2011 Cricket World Cup and 2013 ICC Champions Trophy\n",
            "- Career took off after standout performance in 2009 Emerging Players Tournament in Australia\n",
            "- Praised for maturity and humility by teammates and captain MS Dhoni\n",
            "\n",
            "Conclusion:\n",
            "Virat Kohli's remarkable achievements and talent have solidified his status as a cricketing legend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yl2Q_Qbx8mLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "s0oQ68qPUlkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('Plant diseases by leaf classification.pdf')"
      ],
      "metadata": {
        "id": "MQXHz6y4XhBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ],
      "metadata": {
        "id": "Zc8ZD1DAXxbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2d9H85CXYD7p",
        "outputId": "1d332285-b0f6-43dc-a3ce-c17cd83fc1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ResearchArticle\\nDeep Neural Networks Based Recognition of\\nPlant Diseases by Leaf Image Classification\\nSrdjan Sladojevic,1Marko Arsenovic,1Andras Anderla,1\\nDubravko Culibrk,2and Darko Stefanovic1\\n1DepartmentofIndustrialEngineeringandManagement,FacultyofTechnicalSciences,UniversityofNoviSad,\\nT rgDositejaObradovica6,21000N oviSad,Serbia\\n2DepartmentofInformationEngineeringandComputerScience,UniversityofTrento,ViaSommarive9,Povo,38123Trento,Italy\\nCorrespondenceshouldbeaddressedtoAndrasAnderla;andras@uns.ac.rs\\nReceived9February2016;Revised12May2016;Accepted29May2016\\nAcademicEditor:MarcVanHulleCopyright © 2016 SrdjanSladojevicetal. This is an open access article distributed under the Creative Commons Attribution\\nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properlycited.\\nThelatestgenerationofconvolutionalneuralnetworks(CNNs)has achievedimpressiveresultsinthefieldofimageclassification.\\nThis paper is concerned with a new approach to the development of plant disease recognition model, based on leaf image\\nclassification,bytheuseofdeepconvolutionalnetworks.Nove lwayoftrainingandthemethodologyusedfacilitateaquickandeasy\\nsystemimplementationinpractice.Thedevelopedmodelisabletorecognize13differenttypesofplantdiseasesoutofhealthyleaves,\\nwith the ability to distinguish plant leaves from their surroundings. According to our knowledge, this method for plant disease\\nrecognitionhasbeen proposedforthefirsttime.Allessentialstepsrequiredforimplementing thisdiseaserecognition modelarefullydescribedthroughoutthepaper,startingfromgatheringimagesinordertocreateadatabase,assessedbyagriculturalexperts.Caffe,adeeplearningframeworkdevelopedbyBerkleyVisionandLearningCentre,wasusedtoperformthedeepCNNtraining.\\nThe experimental results on the developed model achieved precision between 91% and 98%, for separate class tests, on average\\n96.3%.\\n1. Introduction\\nTh ep r o b l e mo fe ffi c i e n tp l a n td i s e a s ep r o t e c t i o ni sc l o s e l y\\nrelatedtotheproblemsofsustainableagricultureandclimate\\nchange [1]. Research results indicate that climate change can\\nalter stages and rates of pathogen development; it can alsomodifyhostresistance,whichleadstophysiologicalchangesof host-pathogen interactions [2, 3]. The situation is furthercomplicated by the fact that, today, diseases are transferredglobally more easily than ever before. New diseases canoccurinplaceswheretheywerepreviouslyunidentifiedand,inherently, where there is no local expertise to combat them[4–6].\\nInexperiencedpesticideusagecancausethedevelopment\\nof long-term resistance of the pathogens, severely reducingthe ability to fight back. Timely and accurate diagnosis ofplant diseases is one of the pillars of precision agriculture[7].Itiscrucialtopreventunnecessarywasteoffinancialandother resources, thus achieving healthier production, by\\naddressing the long-term pathogen resistance developmentproblemandmitigatingthenegativeeffectsofclimatechange.\\nIn this changing environment, appropriate and timely\\ndisease identification including early prevention has neverbeen more important. There are several ways to detect plantpathologies.Somediseasesdonothaveanyvisiblesymptoms,or the effect becomes noticeable too late to act, and in those\\nsituations, a sophisticated analysis is obligatory. However,\\nmostdiseasesgeneratesomekindofmanifestationinthevis-iblespectrum,sothenakedeyeexaminationofatrainedpro-fessionalistheprimetechniqueadoptedinpracticeforplantdisease detection. In order to achieve accurate plant diseasediagnosticsaplantpathologistshouldpossessgoodobserva-tion skills so that one can identify characteristic symptoms[8]. Variations in symptoms indicated by diseased plantsmay lead to an improper diagnosis since amateur gardenersand hobbyists could have more difficulties determining it\\nHindawi Publishing Corporation\\nComputational Intelligence and Neuroscience\\nVolume 2016, Article ID 3289801, 11 pages\\nhttp://dx.doi.org/10.1155/2016/32898012 ComputationalIntelligenceandNeuroscience\\nthan a professional plant pathologist. An automated system\\ndesignedtohelpidentifyplantdiseasesbytheplant’sappear-anceandvisualsymptomscouldbeofgreathelptoamateursin the gardening process and also trained professionals as averificationsystemindiseasediagnostics.\\nAdvances in computer vision present an opportunity to\\nexpand and enhance the practice of precise plant protection\\nandextendthemarketofcomputervisionapplicationsinthefieldofprecisionagriculture.\\nExploiting common digital image processing techniques\\nsuch as colour analysis and thresholding [9] were used withtheaimofdetectionandclassificationofplantdiseases.\\nVarious different approaches are currently used for\\ndetecting plant diseases and most common are artificialneural networks (ANNs) [10] and Support Vector Machines(SVMs) [11]. They are combined with different methods ofimagepreprocessinginfavourofbetterfeatureextraction.\\nIn machine learning and cognitive science, ANN is an\\ninformation-processing paradigm that was inspired by theway biological nervous systems, such as the brain, processinformation. The brain is composed of a large number ofhighly interconnected neurons working together to solvespecificproblems.\\nAn artificial neuron is a processing element with many\\ninputs and one output. Although artificial neurons can havemanyoutputs,onlythosewithexactlyoneoutputwillbecon-sidered.Theirinputscanalsotakeonanyvaluebetween0and1.Also,theneuronhasweightsforeachinputandanoverallbias.\\nThe weights are real numbers expressing importance of\\nt h er e s p e c t i v ei n p u t st ot h eo u t p u t .Th eb i a si su s e df o r\\ncontrolling how easy the neuron is getting to output 1. For aneuronwithreallybigbiasitiseasytooutput1,butwhenthebiasisverynegativethenitisdifficulttooutput1.\\nThe output of the neuron is not 0 or 1. Instead, it is 𝛼⋅\\n(𝑤⋅𝑥+𝑏) ,wher e𝛼iscalledthetransferfunction.Thereare\\ndifferenttypesoftransferfunction:step,linear,sigmoid,andsoforth.Thesmoothnessof𝛼meansthatsmallchangesΔ𝑤\\n𝑗\\nin the weights andΔ𝑏in the bias will produce small change\\nΔoutputintheoutputfromtheneuron.Smalloutputchange\\nisapproximatedby\\nΔoutput≈∑𝜕output\\n𝜕𝑤𝑗Δ𝑤𝑗+∑𝜕output\\n𝜕𝑏Δ𝑏.(1)\\nBasically,thesmallchangeinweightorbiascausesthesmall\\ncorrespondingchangeinthenetworkoutput(Figure1).\\nNeuralnetworks,withtheiroutstandingabilitytoderive\\nmeaningfromcomplexorimperfectdata,canbeappliedforextractingpatternsanddetectingtrendsthataretoodifficultto notice by humans or computer techniques. Other advan-tages of ANNs are adaptive learning, self-organization, realtimeoperations,andsoforth.\\nThere are two main categories of ANNs when speaking\\nabout architecture: feed-forward ANNs where the output ofanylayerisunlikelytoinfluencethatsamelayerandfeedbackANNs where signals travel in both directions by involvingloopsinthenetwork.Hiddenw+Δw\\nInputOutputOutput+Δoutput\\nFigure1:SimplemodelofANN.\\nThe method described in this paper is a new approach\\nin detecting plant diseases using the deep convolutionalneuralnetworktrainedandfine-tunedtofitaccuratelytothedatabase of a plant’s leaves that was gathered independentlyfor diverse plant diseases. The advance and novelty of thedeveloped model lie in its simplicity; healthy leaves andbackground images are in line with other classes, enablingthemodeltodistinguishbetweendiseasedleavesandhealthyonesorfromtheenvironmentbyusingdeepCNN.\\nThe rest of the paper is organized as follows: Section 2\\npresents related work, Section 3 presents methodology, Sec-tion 4 presents achieved results and related discussion, andfinally,Section5holdsourconclusions.\\n2. Related Work\\nImplementing the appropriate management strategies likefungicide applications, disease-specific chemical applica-tions,andvectorcontrolthroughpesticideapplicationscouldlead to early information on crop health and disease detec-tion.Thiscouldfacilitatethecontrolofdiseasesandimproveproductivity. In [12], authors present, review, and recognizethedemandfordevelopingarapid,cost-effective,andreliablehealth-monitoring sensor that facilitates advancements inagriculture. They described the currently used technologiesthat include spectroscopic and imaging-based and volatileprofiling-based plant disease detection methods for the pur-pose of developing ground-based sensor system to assist in\\nmonitoring health and diseases in plants under field condi-\\ntions.\\nAfter analysis of their work and analysis presented by\\nthe authors of [13–16], it was decided to use image process-ing disease recognition approach among other approachescommonly used for plant disease diagnostics, for instance,double-stranded ribonucleic acid (RNA) analysis, nucleicacidprobes,andmicroscopy.\\nNumerous procedures are currently in use for plant\\ndisease detection applying computer vision. One of them isdisease detection by extracting colour feature as authors in[17] have presented. In this paper YcbCr, HSI, and CIELBcolourmodelswereusedinthestudy;asaresult,diseasespotsComputationalIntelligenceandNeuroscience 3\\nwere successfully detected and remained unaffected by the\\nnoisefromdifferentsources,suchascameraflash.\\nInaddition,plantdiseasedetectioncouldbeachievedby\\nextracting shape features method. Patil and Bodhe appliedthistechniquefordiseasedetectioninsugarcaneleaveswheretheyhaveusedthresholdsegmentationtodetermineleafarea\\nand triangle threshold for lesioning area, getting the average\\naccuracyof98.60%atthefinalexperiments[18].\\nFurthermore, extracting texture feature could be used in\\ndetecting plant diseases. Patil and Kumar proposed a modelfor plant disease detection using texture features such asinertia,homogeneity,andcorrelationobtainedbycalculatingthegraylevelcooccurrencematrixonimage[19].Combinedwith colour extraction, they experimented on detecting dis-easesonmaizeleaves.\\nCombination of all these features provides a robust\\nfeature set for image improvement and better classification.In [20], the authors have presented a survey of well-knownconventionalmethodsoffeatureextraction.Duetotherapidprogress of Artificial Intelligence (AI) science, work in thispaperismainlyfocusedonapplyingthesemethodologiesandtechniques.\\nTherearesomeapproacheswhichapplythefeed-forward\\nbackpropagationofneuralnetworksconsistingofoneinput,oneoutput,andonehiddenlayerfortheneedsofidentifyingthe species of leaf, pest, or disease; this model was proposedby the authors in [21]. They developed a software model, to\\nsuggest remedial measures for pest or disease management\\ninagriculturalcrops.\\nAnother technique proposed by the authors in [22]\\nincorporates the features extracted by Particle Swarm Opti-mization(PSO)[23]andforwardneuralnetworkindirectionofdeterminingtheinjuredleafspotofcottonandimprovingthe accuracy of the system with the final overall accuracy of95%.\\nAlso, detection and differentiation of plant diseases can\\nbe achieved using Support Vector Machine algorithms. Thistechnique was implemented for sugar beet diseases andpresented in [24], where, depending on the type and stageof disease, the classification accuracy was between 65% and90%.\\nLikewise, there are methods that combine the feature\\nextraction and Neural Network Ensemble (NNE) for plantdisease recognition. Through training a definite numberof neural networks and combining their results after that,NNE offers a better generalization of learning ability [25].Such method was implemented only for recognizing tea leaf\\ndiseaseswithfinaltestingaccuracyof91%[26].\\nAnotherapproachbasedonleafimagesandusingANNs\\nas a technique for an automatic detection and classificationofplantdiseaseswasusedinconjunctionwith 𝐾-meansasa\\nclustering procedure proposed by the authors in [27]. ANNconsisted of 10 hidden layers. The number of outputs was 6which was the number of classes representing five diseasesalongwiththecaseofahealthyleaf.Onaverage,theaccuracyofclassificationusingthisapproachwas94.67%.\\nThe authors in [28–31] presented the deep learning\\nmethodsforsolvingmostcomplextasksindifferentareasofresearch in biology, bioinformatics, biomedicine, robotics,\\nand3Dtechnologies.\\nIn our study, we exploit the deep learning method for\\nplant disease recognition, driven by evolvement of deeplearningtechniquesandtheirapplicationinpractice.Exten-sive search of the state-of-the-art literature yielded no evi-\\ndence that researchers explored deep learning approach for\\nplantdiseasesrecognitionfromtheleafimages.Ourmethodof recognition by applying deep CNN is presented in thesectionsbelow.\\n3. Materials and Methods\\nThe entire procedure of developing the model for plantdisease recognition using deep CNN is described further indetail.Thecompleteprocessisdividedintoseveralnecessarystages in subsections below, starting with gathering imagesforclassificationprocessusingdeepneuralnetworks.\\n3.1. Dataset. Appropriate datasets are required at all stages\\nof object recognition research, starting from training phaseto evaluating the performance of recognition algorithms.All the images collected for the dataset were downloadedfrom the Internet, searched by disease and plant name onvarioussourcesindifferentlanguages,suchasLatin,English,German,Serbian,andHungarian.Imagesinthedatasetweregrouped into fifteen different classes. Thirteen classes rep-\\nresented plant diseases which could be visually determined\\nfromleaves.\\nInordertodistinguishhealthyleavesfromdiseasedones,\\none more class was added in the dataset. It contains onlyimages of healthy leaves. An extra class in the dataset withbackgroundimageswasbeneficialtogetmoreaccurateclas-sification.Thus,deepneuralnetworkcouldbetrainedtodif-ferentiate the leaves from the surrounding. The backgroundimages were taken from the Stanford background dataset[32].\\nIn this stage, all duplicated images taken from different\\nsources were removed by developed python script applyingthecomparingprocedure.Thescriptremovedtheduplicatesbycomparingtheimages’metadata:name,size,andthedate.Aftertheautomatedremoval,imageswereassessedbyhumanexpertsinmuchiteration.\\nNext step was to enrich the dataset with augmented\\nimages. The main goal of the presented study is to train thenetwork to learn the features that distinguish one class fromthe others. Therefore, when using more augmented images,t h ec h a n c ef o rt h en e t w o r kt ol e a r nt h ea p p r o p r i a t ef e a t u r e shas been increased. Finally, a database containing 30880images for training and 2589 images for validation has beencreated.TheaugmentationprocessisdescribedinSection3.3.\\nT a b l e1s h o w sa l ls u p p o r t e dd i s e a s e st o g e t h e rw i t ht h e\\nnumberoforiginalimagesandnumberofaugmentedimagesforeveryclassusedastrainingandvalidationdatasetforthediseaseclassificationmodel.\\n3.2. Image Preprocessing and Labelling. Images downloaded\\nfromtheInternetwereinvariousformatsalongwithdifferent4 ComputationalIntelligenceandNeuroscience\\nTable1:Datasetforimageclassificationofleafdisease.\\nClassNumberoforiginal\\nimagesTotal number of images:\\noriginalandaugmentedNumberofimagesfromthe\\ndatasetusedforvalidation\\n(1)H ealth yleaf 565 4523 331\\n(2)Pear,cherry,andpeach,porosity 265 2124 152\\n(3)Peach,powderymildew 108 1296 90\\n(4)P each,Taphrinadeformans 152 1552 156\\n(5)A pple,pear , Erwiniaamylovora 232 2368 205\\n(6)A pple,pear , Venturia 183 2200 151\\n(7)Apple,powderymildew 120 1440 118\\n(8)A pple,Rust 163 1960 163\\n(9)Pair ,Gymnosporangiumsabinae 267 2142 185\\n(10)Pair,grayleafspot 122 1464 198\\n(11)Grapevine,wilt 287 2300 114\\n(12)Grapevine,mites 250 2000 230\\n(13)Grapevine,powderymildew 237 1900 183\\n(14)Grapevine,down ymildew 297 2376 201\\n(15)Backgroundimages 1235 1235 112\\n4483 30880 2589\\nresolutions and quality. In order to get better feature extrac-\\ntion, final images intended to be used as dataset for deepneural network classifier were preprocessed in order to gainconsistency.Furthermore,procedureofimagepreprocessinginvolved cropping of all the images manually, making thesquare around the leaves, in order to highlight the regionof interest (plant leaves). During the phase of collecting theimages for the dataset, images with smaller resolution anddimension less than 500px were not considered as validimages for the dataset. In addition, only the images wherethe region of interest was in higher resolution were markedas eligible candidates for the dataset. In that way, it wasensured that images contain all the needed information forfeature learning. Images used for the dataset were imageresizedto256×256 toreducethetimeoftraining,whichwas\\nautomaticallycomputedbywrittenscriptinPython,usingthe\\nOpenCVframework[33].\\nMany resources can be found by searching across the\\nInternet,buttheirrelevanceisoftenunreliable.Intheinterestof confirming the accuracy of classes in the dataset, initiallygroupedbyakeywordssearch,agriculturalexpertsexaminedleaf images and labelled all the images with appropriate dis-easeacronym.Asitisknown,itisimportanttouseaccuratelyclassifiedimagesforthetrainingandvalidationdataset.Onlyin that way may an appropriate and reliable detecting modelbe developed. In this stage, duplicated images that were leftafter the initial iteration of gathering and grouping imagesinto classes described in Section 3.1 were removed from thedataset.\\n3.3. Augmentation Process. The main purpose of applying\\naugmentation is to increase the dataset and introduce slightdistortion to the images which helps in reducing overfittingduring the training stage. In machine learning, as well asin statistics, overfitting appears when a statistical model\\ndescribes random noise or error rather than underlyingrelationship [34]. The image augmentation contained oneof several transformation techniques including affine trans-formation, perspective transformation, and simple imagerotations. Affine transformations were applied to expresstranslationsandrotations(lineartransformationsandvectoraddition, resp.) [35] where all parallel lines in the originali m a g ea r es t i l lp a r a l l e li nt h eo u t p u ti m a g e .T ofi n dat r a n s -formationmatrix,threepointsfromtheoriginalimagewereneededaswellastheircorrespondinglocationsintheoutputimage.Forperspectivetransformation,a 3×3transformation\\nmatrix was required. Straight lines would remain straightevenafterthetransformation.Fortheaugmentationprocess,simple image rotations were applied, as well as rotations on\\nthedifferentaxisbyvariousdegrees.\\nTransformations applied in augmentation process are\\nillustratedinFigure2,wherethefirstrowrepresentsresultingimages obtained by applying affine transformation on thesingle image; the second row represents images obtainedfromperspectivetransformationagainsttheinputimageandthelastrowvisualizesthesimplerotationoftheinputimage.Theprocessofaugmentationwaschosentofittheneeds;theleavesinanaturalenvironmentcouldvaryinvisualperspec-tive.\\nForthisstage,inordertoautomatetheaugmentationpro-\\ncess fornumerousimages fromthedataset, particularappli-cationwasdevelopedinC++usingtheOpenCVlibrary[36],withpossibilityofchangingtheparametersoftransformationduringtherun-time,whichimprovesflexibility.\\n3.4. Neural Network Training. Training the deep convolu-\\ntional neural network for making an image classificationmodelfromadatasetdescribedinSection3.1wasproposed.ComputationalIntelligenceandNeuroscience 5\\nAffine\\ntransformation\\n(a)\\nPerspective\\ntransformation\\n(b)\\nRotation\\n(c)\\nFigure2:Imagetransformationsusedforaugmentation:(a)affinetransformations;(b)perspectivetransformations;(c)rotations.\\nThere are several well-known state-of-the-art deep learning\\nframeworks,suchasPythonlibraryTheano[37]andmachinelearning library that extends Lua, Torch7 [38]. In addition,t h e r ei sC a ff e ,a no p e ns o u r c ed e e pl e a r n i n gf r a m e w o r kdevelopedbytheBVLC[39]containingreferencepretrainedCaffeNet model. For the purpose of this research, thisframework was used, along with the set of weights learnedonaverylargedataset,ImageNet[40].\\nCaffe framework is suitable for both research experi-\\nments and industry deployment. The core of framework isd ev e l o pedi nC + +a n dp r o v i d e sc o m m a n dl i n e ,Py th o n ,a n dMATLAB interfaces. Caffe’s integrationwith cuDNN libraryaccelerates Caffe models [41, 42]. CaffeNet is a deep CNN\\nwhichhasmultiplelayersthatprogressivelycomputefeaturesfrom input images [43]. Specifically, the network containseight learning layers and five convolutional and three fullyconnectedlayers[44].\\nCaffeNet architecture is considered a starting point, but\\nmodified and adjusted to support our 15 categories (classes).Lastlayerwasalteredandtheoutputofthesoftmaxlayerwasparameterizedtotherequirementsofpresentedstudy.\\nThe convolutional layer is the essential building block of\\ntheconvolutionalneuralnetwork.Thelayer’sparametersarecomprisedofasetoflearnablekernelswhichpossessasmall6 ComputationalIntelligenceandNeuroscience\\nreceptivefieldbutextendthroughthefulldepthoftheinput\\nvolume[45].\\nEach convolutional layer has𝑀maps of equal size,𝑀𝑥\\nand𝑀𝑦, and a kernel of size𝐾𝑥,a n d𝐾𝑦is shifted over the\\ncertainregionoftheinputimage.Theskippingfactors 𝑆𝑥and\\n𝑆𝑦definehowmanypixelsthefilter/kernelskipsin 𝑥-and𝑦-\\ndirection between subsequent convolutions [46]. The size oftheoutputmapcouldbedefinedas\\n𝑀𝑛\\n𝑥=𝑀𝑛−1\\n𝑥−𝐾𝑛\\n𝑥\\n𝑆𝑛\\n𝑥+1+1,\\n𝑀𝑛\\n𝑦=𝑀𝑛−1\\n𝑦−𝐾𝑛\\n𝑦\\n𝑆𝑛\\n𝑦+1+1,(2)\\nwhere𝑛indicatesthelayer.Eachmapinlayer 𝐿𝑛isconnected\\ntomost𝑀𝑛−1mapsinlayer𝐿𝑛−1.\\nRectified Linear Units (ReLU) are used as substitute for\\nsaturatingnonlinearities.Thisactivationfunctionadaptivelylearns the parameters of rectifiers and improves accuracy atnegligibleextracomputationalcost[47].Itisdefinedas\\n𝑓(𝑧\\n𝑖)=max(0,𝑧𝑖), (3)\\nwhere𝑧𝑖represents the input of the nonlinear activation\\nfunction𝑓onthe𝑖thchannel.\\nDeep CNN with ReLUs trains several times faster. This\\nmethod is applied to the output of every convolutional andfullyconnectedlayer.\\nDespite the output, the input normalization is not\\nrequired; it is applied after ReLU nonlinearity after the first\\nand second convolutional layer because it reduces top-1 and\\ntop-5errorrates.InCNN,neuronswithinahiddenlayeraresegmentedinto“featuremaps.”Theneuronswithinafeaturemapsharethesameweightandbias.Theneuronswithinthefeature map search for the same feature. These neurons areunique since they are connected to different neurons in thelower layer. So for the first hidden layer, neurons within afeaturemapwillbeconnectedtodifferentregionsoftheinputimage.Thehiddenlayerissegmentedintofeaturemapswhereeach neuron in a feature map looks for the same feature butatdifferentpositionsoftheinputimage.Basically,thefeaturemap is the result of applying convolution across an image.Eachlayer’sfeaturesaredisplayed inadifferentblock, wherevisualization represents the strongest activation for the pro-videdfeaturemap,startingfromthefirstconvolutionallayer,wherefeaturesgofromindividualpixelstosimplelines,tothefifth convolutional layer where learned features like shapesandcertainpartsofleavesaredisplayed(Figure3).\\nAnother important layer of CNNs is the pooling layer,\\nwhich is a form of nonlinear downsampling. Pooling oper-ationgivestheformoftranslationinvariance[48];itoperatesindependentlyoneverydepthsliceoftheinputandresizesitspatially.Overlappingpoolingisbeneficiallyappliedtolesseno v e r fi t t i n g .A l s oi nf a v o u ro fr e d u c i n go v e r fi t t i n g,ad r o po u tlayer [49] is used in the first two fully connected layers. Butthe shortcoming of dropout is that it increases training time2-3timescomparingtoastandardneuralnetworkoftheexactarchitecture [50]. Bayesian optimization experiments also\\nproved that ReLUs and dropout have synergy effects, whichm e a n st h a ti ti sa d v a n t a g e o u sw h e nt h e ya r eu s e dt o g e t h e r[51].\\nThe advance of CNNs refer to their ability to learn\\nrich mid-level image representations as opposed to hand-designedlow-levelfeaturesusedinotherimageclassificationmethods[52].\\nFigure 4 illustrates the filtered output images after every\\nconvolutionalandpoolinglayerofthedeepnetwork.Outputimages are labelled with the name of corresponding layer at\\nthebottomrightcornerofeveryimage.\\n3.5. Performed Tests. The common approach in measuring\\nperformanceofartificialneuralnetworksissplittingdataintothe training set and the test set and then training a neuralnetwork on the training set and using the test set for pre-diction. Thus, since the original outcomes for the testing setand our model predicted outcomes are known, the accuracyof our predictioncan be calculated. Different tests were per-formed with 2589 original images, when trained with 30880imagesfromdatabase.\\nFor the accuracy test, 10-fold cross validation technique\\nwasusedtoevaluateapredictivemodel.Thecrossvalidation\\nprocedure was repeated after every thousand training itera-tion.Overallestimatedresultofthetestisgraphicallyrepre-sentedastop-1,totestifthetopclass(theonehavingthehigh-estprobability)isthesameasthetargetlabel.Thetop-5errorrateistheretotestifthetargetlabelisoneofthetop5predic-tions,theoneswith5ofthehighestprobabilities.ThenumberofimagesusedforthevalidationtestfromeachlabelledclassisgiveninTable1.TestresultsarepresentedinSection4,forbothcompletedatasetandeachclassseparately.\\n3.6.Fine-Tuning. Fine-tuningseekstoincreasetheeffective-\\nness or efficiency of a process or function by making smallmodificationstoimproveoroptimizetheoutcome.Theclas-sificationfunctionintheoriginalCaffeNetmodelissoftmaxclassifierthatcomputesprobabilityof1,000classesoftheIma-geNetdataset.Fine-tunedlearningexperimentsrequireabitof learning, but they are still much faster than learning fromscratch[53].Tostartthefine-tuningprocedure,thissoftmaxclassifier was removed, as mentioned and illustrated in Sec-tion3.4andthenewonewasinitializedwithrandomvalues.\\nThe new softmax classifier was trained from scratch using\\nthe back-propagation algorithm with data from the datasetdescribed in Section 3.1. This dataset has 15 different cate-gories[43].Duetothesmallersizeofthedatasetusedforthisresearch when compared to ImageNet, ILSVRC-2012 [54],overfitting was constrained by using lower initial learningrates for the fine-tuned hidden layers [55]. The learning rateofthetoplayerwassetto10,whilethelearningrateofalltheother seven learning layers was 0.1. The back-propagationalgorithm ran for 100,000 iterations. The process of fine-tuning was repeated changing parameters of hidden layersandhyperparameters.Thebestsuitedmodelforplantdiseasedetection was achieved through the process of experimentalComputationalIntelligenceandNeuroscience 7\\n50 100 150 2000200150100500\\n(a)\\n100806040200\\n20 40 60 80 1000\\n(b)\\n50100 150 200 250 300 0300250200150100500\\n(c)\\n50100150200250 0250200150100500\\n(d)\\n150100500\\n150100500\\n(e)\\n50 100 150 200 2500250200150100500\\n(f)\\n50100150 200 250 0250200150100500\\n(g)\\n50100 150 200 0200150100500\\n(h)\\nFigure3:Visualizationoffeaturesintrainedclassificationmodel:(a)originalimage;(b)thefirstlayerfilters,Conv1;(c)thefirstlayeroutput,\\nConv1rectifiedresponsesofthefilters,first36only;(d)thesecondlayerfilters,Conv2;(e)thesecondlayeroutput,Conv2(rectified,onlythefirst 36 of 256 channels); (f) the third layer output, Conv3 (rectified, all 384 channels); (g) the fourth layer output, Conv4 (rectified, all 384\\nchannels);(h)thefifthlayeroutput,Conv5(rectified,all256channels).\\nConv1 Pool1 Norm1 Conv2 Pool2\\nNorm1 Conv3 Conv4 Conv5 Pool5\\nFigure4:Outputlayersimages.\\nadjustment of the parameters. The results of the model fine-\\ntuningarepresentedandexplainedfurtherinSection4.\\n3.7. Equipment. A single PC was used for the entire process\\nof training and testing the plant disease detection model\\ndescribedinthispaper.TrainingoftheCNNwasperformed\\nin Graphics Processing Unit (GPU) mode. Every training\\niteration took approximately eight hours on this specified\\nmachinewhosebasiccharacteristicsarepresentedinTable2.4. Results and Discussion\\nThe results presented in this section are related to training\\nwith the whole database containing both original and aug-m e n t e di m a g e s .A si ti sk n o w nt h a tc o n v o l u t i o n a ln e t w o r k sare able to learn features when trained on larger datasets,results achieved when trained with only originalimages willnotbeexplored.\\nAfterfine-tuningtheparametersofthenetwork,anover-\\nall accuracy of 96.3% was achieved, after the 100th training8 ComputationalIntelligenceandNeuroscience\\nTable2:Basicmachinecharacteristics.\\nHardwareandsoftware Characteristics\\n(1)M emory 16Gb\\n(2)Processor IntelCorei7-4790CPU@3.60GHz ×8\\n(3)Graphics GeForceGTXTITANX12Gb\\n(4)Operatingsystem LinuxUbuntu14.0464bits\\nAccuracy and loss through the iterations\\nTraining log loss\\nValidation accuracy200000 60000 80000 100000 40000\\nIterations0.00.20.40.60.81.0\\nValidation accuracy (%)\\n0.00.51.01.52.02.53.03.5Loss (%)\\nFigure5:Accuracyofthefine-tunedCNN.\\niteration (95.8% without fine-tuning). Even after the 30th\\ntraining iteration high accuracy results were achieved withexceedingly reduced loss, but after the 60th iteration, thebalanceinaccuracyandlosswascarriedoutinhighaccuracy.The green line in the graph in Figure 5 shows the network’ssuccessonthevalidationtestset,throughtrainingiterations.After every 10 thousand training iterations, the snapshot ofthemodelwasobtained.Thebluelineinthegraphrepresentsthelossduringthetrainingstage.Throughtrainingiterations,losswasrapidlyreduced.\\nTop-1 success was 96.3% and top-5 success was 99.99%\\nafter 100,000 iterations which are shown in Figures 6 and 7,respectively.\\nFurthermore,thetrainedmodelwas tested on each class\\nindividually.Testwasperformedoneveryimagefromtheval-idationset.Theresultsaredisplayedtoemphasizehowmanyima g esfr o mt o talo feac hc las sa r eaccura t el yp r edict ed .Fig-ure 8 illustrates trained model’s prediction results separatedfor every class. The class numbers follow enumeration from\\nTable 1.\\nFrom the results displayed in Figure 8, it is notable that\\nthetrainedmodel’saccuracywasslightlylessforclasseswithlowernumberofimagesinthetrainingdataset,morespecifi-callyclassespeach,powderymildew,apple,powderymildew,and grapevine, wilt. Achieved accuracy was in range from91.11% for peach, powdery mildew, up to 98.21% for back-groundimages.Highaccuracyofmodel’spredictionofback-groundimagesallowsgoodseparationofplantsleavesandthesurroundings.\\nAssuggestedbygoodpracticeprinciples,achievedresults\\nshould be compared with some other results. Taking intoaccount the fact that during this research our own imageTop-1through the iterations\\n0.00.20.40.60.81.0Top-1(%) of success\\n60000 40000 80000 100000 20000 0\\nIterations\\nFigure6:Top-1accuracysuccess.\\nTop-5through the iterations\\n0 40000 60000 80000 100000 20000\\nIterations0.60.70.80.91.0Top-5(%) of success\\n0.5\\nFigure7:Top-5accuracysuccess.\\nd a t a b a s ew a sd e v e l o p e d ,n oo n eh a su s e di tu pt on o w .I n\\naddition,sincenoonehasuseddeeplearningtoidentifyplantdiseases in scientific literature, it is impossible to compareit with other examples. Nonetheless, as a result of extensivereview, deep learning techniques have showed better resultsinpatternrecognition,intheimagesegmentationandobjectdetection.Thisisalsoproveninpracticebynumerouscompe-titionswonbyconvolutionalneuralnetworks[56].Presently,there is a commercial solution, Leafsnap [57], which usesvisualrecognitioninordertoidentifytreespeciesfromtheirleaves’ images but as the network presented in this paperis classifying the plant diseases instead of types of plant,\\nLeafsnapwasnotusedforcomparisonoftheachievedresults.\\nFinally,comparingourresultswithothermethodsofdetect-ingdiseasesfromleavesimages,itcanbesaidthatourmethodprovidesbetterresults[23,24,26,27].\\n5. Conclusions\\nThere are many methods in automated or computer visionplantdiseasedetectionandclassificationprocess,butstill,thisresearch field is lacking. In addition, there are still no com-mercial solutions on the market, except those dealing withplantspeciesrecognitionbasedontheleavesimages.\\nIn this paper, a new approach of using deep learning\\nmethod was explored in order to automatically classify anddetectplantdiseasesfromleafimages.Thedevelopedmodelwas able to detect leaf presence and distinguish betweenhealthyleavesand13differentdiseases,whichcanbevisuallyComputationalIntelligenceandNeuroscience 9\\nTesting accuracy\\nAll imagesClasses\\nAccurately predicted1234567 8 91 01 11 21 31 4 1596.37 %\\n96.05 %\\n91.11 %94.87 %96.59 %\\n94.7%\\n91.53 %95.09 %97.3%91.92 %\\n96.49 %96.09 %\\n96.17 %97.01 %\\n98.21 %\\n050100150200250300350Number of images\\nFigure8:Predictionaccuracyforeachclassseparately.\\ndiagnosed. The complete procedure was described, respec-\\ntively, from collecting the images used for training and val-idationtoimagepreprocessingandaugmentationandfinally\\ntheprocedureoftrainingthedeepCNNandfine-tuning.Dif-\\nferenttestswereperformedinordertochecktheperformanceofnewlycreatedmodel.\\nNew plant disease image database was created, con-\\ntaining more than 3,000 original images taken from theavailableInternetsourcesandextendedtomorethan30,000using appropriate transformations. The experimental resultsachieved precision between 91% and 98%, for separate classtests. The final overall accuracy of the trained model was96.3%.Fine-tuninghasnotshownsignificantchangesintheoverallaccuracy,butaugmentationprocesshadgreaterinflu-encetoachieverespectableresults.\\nAsthepresentedmethodhasnotbeenexploited,asfaras\\nwe know, in the field of plant disease recognition, there wasnocomparisonwithrelatedresults,usingtheexacttechnique.In comparison with other techniques used and presented inSection 2, comparable or even better results were achieved,especially when taking into account the wider number ofclassesinthepresentedstudy.\\nAnextensionofthisstudywillbeongatheringimagesfor\\nenrichingthedatabaseandimprovingaccuracyofthemodelusingdifferenttechniquesoffine-tuningandaugmentation.\\nThe main goal for the future work will be developing a\\ncomplete system consisting of server side components con-\\ntaining a trained model and an application for smart mobile\\ndevices with features such as displaying recognized diseasesin fruits, vegetables, and other plants, based on leaf imagescaptured by the mobile phone camera. This application willserve as an aid to farmers (regardless of the level of experi-ence),enablingfastandefficientrecognitionofplantdiseasesand facilitating the decision-making process when it comestotheuseofchemicalpesticides.\\nFurthermore, future work will involve spreading the\\nusageofthemodelbytrainingitforplantdiseaserecognitionon wider land areas, combining aerial photos of orchardsand vineyards captured by drones and convolution neuralnetworks for object detection. By extending this research,theauthorshopetoachieveavaluableimpactonsustainable\\ndevelopment,affectingcropqualityforfuturegenerations.\\nCompeting Interests\\nThe authors declare that there is no conflict of interestsregardingthepublicationofthispaper.\\nAcknowledgments\\nThe research presented in this paper was supported by FP7IRSESProjectQoSTREAM.\\nReferences\\n[1] K. A. Garrett, S. P. Dendy, E. E. Frank, M. N. Rouse, and S. E.\\nTravers, “Climate change effects on plant disease: genomes toecosystems,” AnnualReviewofPhytopathology ,vol.44,pp.489–\\n509,2006.\\n[2] S.M.Coakley ,H.Scherm,andS.Chakraborty ,“Climatechange\\nandplantdiseasemanagement,” AnnualReviewofPhytopathol-\\nogy,vol.37 ,no .1,pp .399–426,1999 .\\n[ 3 ]S .C h a k r a b o r t y ,A .V .T i e d e m a n n ,a n dP .S .T e n g ,“ C l i m a t e\\nchange:potentialimpactonplantdiseases,” EnvironmentalPol-\\nlution,vol.108,no .3,pp .317 –326,2000.\\n[ 4 ]A .J .T a t e m ,D .J .R o g e r s ,a n dS .I .H a y ,“ G l o b a lt r a n s p o r tn e t -\\nw o r ksa n dinf ectio u sdi sea ses p r ead , ” Advances in Parasitology ,\\nvol.62,pp.293–343,2006.\\n[ 5 ]J .R.R o h r ,T .R.Ra ff e l ,J .M .R o m a n s i c ,H .M c Ca l l u m ,a n dP .J .\\nHudson,“Evaluatingthelinksbetweenclimate,diseasespread,and amphibian declines,” Proceedings of the National Academy\\nof Sciences of the United States of America ,v o l .1 0 5 ,n o .4 5 ,p p .\\n17436–17441,2008.\\n[6] T.VanderZwet,“Presentworldwidedistributionoffireblight,”\\ninProceedings of the 9th International Workshop on Fire Blight ,\\nvol.590,Napier,NewZealand,October2001.\\n[ 7 ]S .A .M i l l e r ,F .D .B e e d ,a n dC .L .H a r m o n ,“ P l a n td i s e a s e\\ndiagnostic capabilities and networks,” Annual Review of Phy-\\ntopathology ,vol.47 ,pp .15–38,2009 .\\n[ 8 ]M .B .R i l e y ,M .R .W i l l i a m s o n ,a n dO .M a l o y ,“ P l a n td i s e a s e\\ndiagnosis.ThePlantHealthInstructor,”2002.\\n[9] J. G. Arnal Barbedo, “Digital image processing techniques for\\ndetecting,quantifyingandclassifyingplantdiseases,” Springer-\\nPlus,vol.2,article660,pp .1 –12,2013.\\n[10] H. Cartwright, Ed., Artificial Neural Networks , Humana Press,\\n2015.\\n[11] I. Steinwart and A. Christmann, Support Vector Machines ,\\nSpringerScience&BusinessMedia,NewYork,NY,USA,2008.\\n[12] S. Sankaran, A. Mishra, R. Ehsani, and C. Davis, “A review of\\nadvanced techniques for detecting plant diseases,” Computers\\nandElectronicsinAgriculture ,vol.72,no .1,pp .1 –13,2010.\\n[ 1 3 ]P .R .R e d d y ,S .N .D i vy a ,a n dR .V i j a y a l a k s h m i ,“ P l a n td i s e a s e\\ndetectiontechniquetool—atheoreticalapproach,” International\\nJournalofInnovativeTechnologyandResearch ,pp .91 –93,2015.\\n[14] A.-K. Mahlein, T. Rumpf, P. Welke et al., “Development of\\nspectral indices for detecting and identifying plant diseases,”\\nRemoteSensingofEnvironment ,vol.128,pp .21 –30,2013.\\n[15]W .Xiuqing,W .Haiyan,andY .Shifeng,“Plantdiseasedetection\\nbased on near-field acoustic holography,” Transactions of the\\nChinese Society for Agricultural Machinery ,v o l .2 ,a r t i c l e4 3 ,\\n2014.10 ComputationalIntelligenceandNeuroscience\\n[16] A.-K. Mahlein, E.-C. Oerke, U. Steiner, and H.-W. Dehne,\\n“Recent advances in sensing plant diseases for precision crop\\nprotection,” EuropeanJournal ofPlant Pathology ,vol.133,no .1,\\npp .197 –209 ,2012.\\n[17] P.Chaudhary, A. K. Chaudhari,A. N. Cheeran, andS. Godara,\\n“Color transform based approach for disease spot detection\\non plant leaf,” International Journal of Computer Science and\\nTelecommunications ,vol.3,no .6,pp .65–69 ,2012.\\n[18] S.B.PatilandS.K.Bodhe,“Leafdiseaseseveritymeasurement\\nusing image processing,” International Journal of Engineering\\nandTechnology ,vol.3,no .5,pp .297 –301,2011.\\n[19] J. K. Patil and R. Kumar, “Feature extraction of diseased leaf\\nimages,”Journal of Signal & Image Processing , vol. 3, no. 1, p.\\n60,2012.\\n[ 2 0 ]T .R .R e e da n dJ .M .H .D u b u f ,“ Ar e v i e wo fr e c e n tt e x t u r e\\nsegmentationandfeatureextractiontechniques,” CVGIP:Image\\nUnderstanding ,vol.57 ,no .3,pp .359–372,1993.\\n[21] M. S. P. Babu and B. Srinivasa Rao, “Leaves recognition using\\nback propagation neural network-advice for pest and disease\\ncontrol on crops,” I n d i a K i s a n .N e t :E x p e r tA d v i s o r yS y s t e m ,\\n2007.\\n[22] P.RevathiandM.Hemalatha,“Identificationofcottondiseases\\nbased on cross information gain deep forward neural network\\nclassifier with PSO feature selection,” International Journal of\\nEngineeringandTechnology ,vol.5,no .6,pp .4637 –4642,2014.\\n[ 23 ]C .Z h o u ,H .B .G a o ,L .G a o ,a n dW .G .Z h a n g,“ P a rti c l es w a rm\\noptimization (PSO) algorithm,” Application Research of Com-\\nputers,vol.12,pp .7 –11,2003.\\n[24] T.Rumpf,A.-K.Mahlein,U.Steiner,E.-C.Oerke,H.-W.Dehne,\\nand L. Pl ¨umer, “Early detection and classification of plant\\ndiseaseswithSupportVectorMachinesbasedonhyperspectral\\nreflectance,” Computers and Electronics in Agriculture ,v o l .7 4 ,\\nno.1,pp.91–99,2010.\\n[25] Z.H.ZhouandS.F.Chen,“Neuralnetworkensemble,” Chinese\\nJournal of Computers ,vol.25,no .1,pp .1 –8,2002.\\n[26] B. C. Karmokar, M. S. Ullah, Md. K. Siddiquee, and K. Md.\\nR. Alam, “Tea leaf diseases recognition using neural networkensemble,” International Journal of Computer Applications ,v o l .\\n114,no .17 ,pp .27 –30,2015.\\n[27] H.Al-Hiary,S.Bani-Ahmad,M.Reyalat,M.Braik,andZ.AL-\\nRahamneh, “Fast and accurate detection and classification ofplantdiseases,” MachineLearning ,vol.14,p.5,2011.\\n[28] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting\\nrobotic grasps,” The International Journal of Robotics Research ,\\nvol.34,no.4-5,pp.705–724,2015.\\n[29] B. Alipanahi, A. Delong, M. T. Weirauch, and B. J. Frey, “Pre-\\ndicting the sequence specificities of DNA- and RNA-bindingproteinsbydeeplearning,” Nature Biotechnology ,vol.3 3,no .8,\\npp .831 –838,2015.\\n[30] L.Zhang,G.-S.Xia,T.Wu,L.Lin,andX.C.Tai,“Deeplearning\\nfor remote sensing image understanding,” Journal of Sensors ,\\nvol.2016,ArticleID7954154,2pages,2016.\\n[31] J. Arevalo, F. A. Gonzalez, R. Ramos-Pollan, J. L. Oliveira, and\\nM. A. G. Lopez, “Convolutional neural networks for mam-\\nmographymasslesionclassification,”in Proceedingsof the37th\\nAnnual International Conference of the IEEE Engineering in\\nMedicine and Biology Society (EMBC ’15) ,p p .7 9 7 – 8 0 0 ,A u g u s t\\n2015.\\n[32] S. Gould, R. Fulton, and D. Koller, “Decomposing a scene into\\ngeometric and semantically consistent regions,” in Proceedings\\nofthe12thInternationalConferenceonComputerVision(ICCV\\n’09), pp. 1–8, Kyoto, Japan, October 2009.[33] J.Howse, OpenCVComputerVisionwithPython ,PacktPublish-\\ning,Birmingham,UK,2013.\\n[34] D.M.Hawkins,“Theproblemofover-fitting,” JournalofChem-\\nical Information and Computer Sciences ,v o l .44 ,n o .1 ,p p .1 – 12 ,\\n2004.\\n[35] C. C. Stearns and K. Kannappan, “Method for 2-D affine\\ntransformationofimages,”USPatentNo.5,475,803,1995.\\n[36] S.Brahmbhatt, PracticalOpenCV ,A press,2013.\\n[37] J.Bergstra,F.Bastien,O.Breuleuxetal.,“Theano:deeplearning\\non gpus with python,” in Proceedings of the NIPS 2011, Big\\nLearningWorkshop ,Granada,Spain,December2011.\\n[38] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: a\\nmatlab-like environment for machine learning,” BigLearn,\\nNIPSWorkshopEPFL-CONF-192376,2011.\\n[39] Y. Jia, E. Shelhamer, J. Donahue et al., “Caffe: convolutional\\narchitecture for fast feature embedding,” in Proceedings of the\\nACM Conference on Multimedia (MM ’14) ,pp. 675–678, ACM,\\nOrlando,Fla,USA,November2014.\\n[40] D.Jia,W.Dong,R.Socheretal.,“ImageNet:alarge-scalehierar-\\nchicalimagedatabase,”in ProceedingsoftheIEEEConferenceon\\nComputerVisionandPatternRecognition(CVPR’09) ,pp .248–\\n255,Miami,Fla,USA,June2009.\\n[41] Tips, CUDA Pro, and C. U. D. A. Spotlights, “Deep Learning\\nf o rC o m p u t e rV i s i o nw i t hC a ff ea n dc u D N N , ”F e b r u a r y2 0 1 6 ,\\nhttps://devblogs.nvidia.com/parallelforall/deep-learning-com-\\nputer-vision-caffe-cudnn/.\\n[42] S. Bahrampour, N. Ramakrishnan, L. Schott, and M. Shah,\\n“Comparativestudyofcaffe,neon,theano,andtorchfordeep15\\nlearning,”http://arxiv.org/abs/1511.06435v1.\\n[43] A.K.Reyes,J.C.Caicedo,andJ.E.Camargo,“Fine-tuningdeep\\nconvolutionalnetworksforplantrecognition,”in Proceedingsof\\ntheWorkingNotesofCLEF2015Conference ,2015,http://ceur-ws\\n.org/Vol-1391/121-CR.pdf.\\n[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet Classi-\\nficationwithDeepConvolutionalNeuralNetworks ,Advancesin\\nNeuralInformationProcessingSystems,2012.\\n[45] G.Montavon,M.L.Braun,andK.-R.M ¨uller,“Kernelanalysisof\\ndeepnetworks,” TheJournalofMachineLearningResearch ,vol.\\n12,pp.2563–2581,2011.\\n[46] D. C. Cires ¸an, U. Meier, J. Masci, L. M. Gambardella, and J.\\nSchmidhuber, “Flexible, high performance convolutional neu-\\nralnetworksforimageclassification,”in Proceedingsofthe22nd\\nInternationalJointConferenceonArtificialIntelligence ,vol.2,pp.\\n1237–1242,2011.\\n[47] C. Ciresan Dan, U. Meier, J. Masci, L. M. Gambardella, and J.\\nSchmidhuber, “Flexible, high performance convolutional neu-\\nralnetworksforimageclassification,”in ProceedingsoftheInter-\\nnationalJointConferenceonArtificialIntelligence(IJCAI’11) ,vol.\\n22,no.1,pp.1237–1242,2011.\\n[48] A. Romero, Assisting the training of deep neural networks with\\napplications to computer vision [Ph.D. thesis] , Universitat de\\nBarcelona,Barcelona,Spain,2015.\\n[49] S.Han,J.Pool,J.Tran,andW.J.Dally,“Learningbothweights\\nandconnectionsforefficientneuralnetwork,”in Proceedingsof\\nthe Advances in Neural Information Processing Systems (NIPS\\n’05),NIPSProceedings,pp.1135–1143,2015.\\n[50] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\\nR. Salakhutdinov, “Dropout: a simple way to prevent neural\\nnetworks from overfitting,” The Journal of Machine Learning\\nResearch,vol.15,no .1,pp .1929–1958,2014.ComputationalIntelligenceandNeuroscience 11\\n[ 5 1 ]G .E .D a h l ,T .N .S a i n a t h ,a n dG .E .H i n t o n ,“ I m p r o v i n gd e e p\\nneural networks for LVCSR using rectified linear units and\\ndropout,” in P r o c e e d i n g so ft h e38 t hI E E EI n t e r n a t i o n a lC o n f e r -\\nenceonAcoustics,Speech,andSignalProcessing(ICASSP’13) ,pp.\\n8609–8613,Vancouver,Canada,May2013.\\n[52] M.Oquab,L.Bottou,I.Laptev,andJ.Sivic,“Learningandtrans-\\nferring mid-level image representations using convolutional\\nneuralnetworks,”in Proceedingsofthe27thIEEEConferenceon\\nComputerVisionandPatternRecognition(CVPR’14) ,pp.1717–\\n1724,IEEE,Columbus,Ohio,USA,June2014.\\n[53] A.Sethi,“ExperimentswithFine-tuningCaffeModels”.\\n[ 5 4 ]O .R u s s a k o v s k y ,J .D e n g ,H .S ue ta l . ,“ I m a g e n e tl a r g es c a l e\\nvisualrecognitionchallenge,” InternationalJournalofComputer\\nVision,vol.115,no .3,pp .211 –252,2015.\\n[55] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman,\\n“Return of the devil in the details: delving deep into convolu-\\ntionalnets,”http://arxiv.org/abs/1405.3531.\\n[56] J. Schmidhuber, “Deep Learning in neural networks: an\\noverview,” Neural Networks ,vol.61,pp.85–117,2015.\\n[57] “Leafsnap: An Electronic Field Guide,” 2016, http://leafsnap\\n.com//.Submit your manuscripts at\\nhttp://www.hindawi.com\\nComputer Games  \\n TechnologyInternational Journal of\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014Distributed  \\n Sensor NetworksInternational Journal of\\nAdvances in\\nFuzzy\\nSystems\\nHindawi Publishing Corporation\\nhttp://www.hindawi.comVolume 2014\\nInternational Journal of\\nReconfigurable\\nComputing\\nHindawi Publishing Corporation \\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\n Applied \\nComputational \\nIntelligence and Soft Computing\\n\\u2009Advances \\u2009in\\u2009\\nArtificial \\nIntelligence\\nHindawi\\u2009Publishing \\u2009Corporation\\nhttp://www.hindawi.com Volume\\u20092014\\nAdvances in\\nSoftware Engineering\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014Electrical and Computer \\nEngineeringJournal of\\nJournal of\\nComputer Networks \\nand Communications\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014 Advances in \\nMultimedia\\n International Journal of \\nBiomedical Imaging\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nArtificial\\nNeural Systems\\nAdvances in\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com\\n Volume 201 4\\nRoboticsJournal of\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014Computational \\nIntelligence and Neuroscience\\nIndustrial EngineeringJournal of\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014\\nModelling & \\nSimulation in Engineering\\nHindawi Publishing Corporation \\nhttp://www.hindawi.com Volume 2014\\nThe Scientific  \\nWorld Journal\\nHindawi Publishing Corporation \\nhttp://www.hindawi.com Volume 2014\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014Human-Computer\\nInteractionAdvances in\\nComputer EngineeringAdvances in\\nHindawi Publishing Corporation\\nhttp://www.hindawi.com Volume 2014'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ2JMtNHYHOt",
        "outputId": "1f539b1b-321a-4df2-975f-701d127185dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 935, which is longer than the specified 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adoroLzFYjXR",
        "outputId": "1ba310c4-4060-48c5-d358-4a113c6bb36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "lKjrdpFmYqhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "OMNtp15mYtzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwertVc-YzmP",
        "outputId": "50ff4393-edd0-4e49-bf18-e2955fde2c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"give the summary for plant diseases by leaf classification\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "qdKBbgyuY2ui",
        "outputId": "bec55235-8400-4da3-d51c-3006f9e39b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe paper describes a new approach using deep learning to automatically classify and detect plant diseases from leaf images. The developed model was able to recognize 13 different types of plant diseases and distinguish them from healthy leaves. The entire process, from collecting images to training and validation, was explained in detail. The experimental results showed an average precision of 96.3% for separate class tests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C009YVAPZIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import OnlinePDFLoader"
      ],
      "metadata": {
        "id": "e5Y5tGi5Zi_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")"
      ],
      "metadata": {
        "id": "fGhE_sbnaABJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7Zw-lWsaC6Q",
        "outputId": "85913c04-e863-4877-81fa-78fd6fd992fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.OnlinePDFLoader at 0x7c42fc1b6ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4vOFezgaM9X",
        "outputId": "d3a67d9b-67a9-487c-9541-265110b9440c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.12.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.4)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.0)\n",
            "Collecting unstructured-client>=0.15.1 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.3.2)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.6)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.20.2)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=a3490c09863c2ef63197d7677c8450bb3f8fffad74c1f058b4a7f252baa0d38c\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, jsonpath-python, emoji, dataclasses-json-speakeasy, unstructured-client, unstructured\n",
            "Successfully installed dataclasses-json-speakeasy-0.5.11 emoji-2.10.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.6.1 unstructured-0.12.4 unstructured-client-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdf2image\n",
        "!pip install pdfminer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfmYHcP1aGC_",
        "outputId": "984e6d48-ee36-43c4-90bb-6f0a82263f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140089 sha256=436505f48d3e57bfb62e8c1aa6131fc0237e0d215e690118c2d1c12e36c64a6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "cmdxNR2EaIwU",
        "outputId": "28d83f77-2da5-46f3-d9c4-30ce3b8d15a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'open_filename' from 'pdfminer.utils' (/usr/local/lib/python3.10/dist-packages/pdfminer/utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-c440b526cd76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;34m\"\"\"Load documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnstructuredPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;34m\"\"\"Load file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_process_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elements\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m_get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0munstructured\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartition_pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpartition_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstructured_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unstructured/partition/pdf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdftypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFObjRef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPILImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpillow_heif\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_heif_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'open_filename' from 'pdfminer.utils' (/usr/local/lib/python3.10/dist-packages/pdfminer/utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3n4W5_EUaiRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}